# https://packaging.python.org/en/latest/guides/writing-pyproject-toml/
# https://packaging.python.org/en/latest/tutorials/packaging-projects/

# bash scripts/pypi_achatbot.sh

# https://test.pypi.org/project/achatbot/
# https://pypi.org/project/achatbot/

# pip install pip-tools
# pip-compile --all-extras pyproject.toml

[build-system]
requires = ["setuptools>=61.0", "setuptools-scm>=8.0"]
build-backend = "setuptools.build_meta"

[project]
name = "achatbot"
#dynamic = ["version"]
version = "0.0.21"
authors = [{ name = "weedge", email = "weege007@gmail.com" }]
maintainers = [{ name = "weedge", email = "weege007@gmail.com" }]
description = "An open source chat bot for voice (and multimodal) assistants"
readme = "README.md"
license = { file = "LICENSE" }
keywords = [
    "ai",
    "chat bot",
    "audio",
    "speech",
    "video",
    "image",
    "vision",
    "mcp",
    "avatar",
]
# https://pypi.org/classifiers/
classifiers = [
    # How mature is this project? Common values are
    #   3 - Alpha
    #   4 - Beta
    #   5 - Production/Stable
    "Development Status :: 4 - Beta",

    # Indicate who your project is intended for
    "Intended Audience :: Developers",
    "Topic :: Software Development :: Build Tools",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Multimedia :: Sound/Audio",
    "Topic :: Multimedia :: Video",

    # BSD 3-Clause License
    "License :: OSI Approved :: BSD License",

    # Specify the Python versions you support here.
    "Programming Language :: Python :: 3",
    # when install TTS numpy==1.22.0;python_version<="3.10"
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    "requests",
    "apipeline>=0.2.1",
    "python-dotenv",
    "pydub",            # need install ffmpeg
    "pillow",
    "aiohttp==3.10.11",
    "scipy",
    "pyloudnorm",
    "pydantic",
    "numpy>=1.22.0",
]

[project.urls]
Homepage = "https://github.com/ai-bot-pro/chat-bot"
Documentation = "https://github.com/ai-bot-pro/chat-bot/blob/main/docs"
Repository = "https://github.com/ai-bot-pro/chat-bot.git"
Issues = "https://github.com/ai-bot-pro/chat-bot/issues"
Changelog = "https://github.com/ai-bot-pro/chat-bot/blob/main/CHANGELOG.md"


[project.optional-dependencies]
pyee = ["pyee~=12.0.0"]
conf = ["omegaconf~=2.3.0", "hydra-core~=1.3.2"]
yaml = ["PyYAML~=6.0.2"]
gdown = ["gdown==5.1.0"]
matplotlib = ["matplotlib==3.7.5"]
# base opt dep
tensorrt = ["tensorrt~=10.4.0"]
einops = ["einops~=0.8.0"]
# with --no-build-isolation
flash-attn = ["flash-attn==2.7.3"]
tiktoken = ["tiktoken~=0.7.0"]
verovio = ["verovio~=4.3.1"]
accelerate = ["accelerate~=1.7.0"]
opencv = ["opencv-python~=4.10.0.84"]
librosa = ["librosa~=0.10.2.post1"]
soundfile = ["soundfile~=0.12.1"]
torch_vision_audio = [
    "torch~=2.6.0",
    "torchaudio~=2.6.0",
    "torchvision~=0.21.0",
]
# https://github.com/modelcontextprotocol
# https://github.com/modelcontextprotocol/python-sdk
# https://github.com/jlowin/fastmcp
# https://gofastmcp.com/
mcp = ["mcp[cli]~=1.9.1", "fastmcp"]


# diffusers DiT with quantizing model
diffusers = ["achatbot[torch_vision_audio]", "diffusers[torch]~=0.31.0"]

# quantization
# for transformers model weights with BitsAndBytes to 4bit/8bit quantization
bitsandbytes = ["bitsandbytes~=0.44.1"]
# for transformers awq model
autoawq = ["autoawq"]


# network framework
fastapi = ["fastapi~=0.112.0"]
websocket = ["websockets~=12.0"]
# for simple dummy bot server to test
fastapi_bot_server = ["fastapi~=0.112.0", "uvicorn~=0.30.6"]

# ngrok proxy
ngrok_proxy = ["pyngrok~=7.2.0", "nest-asyncio~=1.6.0"]


# webRTC
agora = [
    #need move colorlog
    "colorlog",
    "agora-realtime-ai-api-v1~=0.0.3",   # dep agora_python_server_sdk_v1
    "agora_python_server_sdk_v1~=0.0.3",
    #"agora_python_server_sdk~=2.2.4",
    "achatbot[opencv]",
]
daily = ["daily-python~=0.11.0"]
livekit = ["livekit~=0.17.5"]
# http api sdk
livekit-api = ["livekit-api~=0.7.1"]
webrtc = ["aiortc~=1.13.0"]

# api llm
google_ai = ["google-generativeai~=0.8.3"]
openai = ["openai~=1.54.1"]
together_ai = ["together~=1.3.3"]
litellm = ["litellm~=1.52.0"]

# rpc
grpc = ["grpcio>=1.65.1"]
grpc_tools = ["grpcio-tools>=1.65.1"]
rpc = ["grpcio>=1.65.1"]

# queue
redis = ["redis~=5.0.0"]
queue = ["achatbot[redis]"]

# transports
livekit_transport = ["achatbot[livekit,livekit-api]"]
daily_transport = ["achatbot[daily]"]
websocket_server_transport = ["achatbot[websocket]"]
agora_transport = ["achatbot[agora]"]

# audio_stream module tag -> pkgs

# PyAudio need install python3-pyaudio 
# e.g. ubuntu `apt-get install python3-pyaudio`, macos `brew install portaudio`
# see: https://pypi.org/project/PyAudio/
pyaudio_stream = ["PyAudio~=0.2.14"]
daily_room_audio_stream = ["achatbot[daily]"]
livekit_room_audio_stream = ["achatbot[livekit,livekit-api]"]
agora_channel_audio_stream = ["achatbot[agora]"]
speech_audio_stream = ["PyAudio~=0.2.14", "daily-python~=0.11.0"]

# waker module tag -> pkgs
porcupine_wakeword = ["pvporcupine~=3.0.2"]
speech_waker = ["achatbot[porcupine_wakeword]"]

# vad module tag -> pkgs
pyannote_vad = ["pyannote.audio~=3.2.0"]
webrtcvad = ["webrtcvad~=2.0.10"]
silero_vad = ["achatbot[torch_vision_audio]"]
webrtc_silero_vad = ["achatbot[webrtcvad,silero_vad]"]
speech_vad = ["achatbot[pyannote_vad,webrtcvad,silero_vad]"]

#speech_vad_analyzer
# vad_analyzer module tag -> pkgs
daily_webrtc_vad_analyzer = ["achatbot[daily]"]
silero_vad_analyzer = ["achatbot[silero_vad]"]
speech_vad_analyzer = [
    "achatbot[daily_webrtc_vad_analyzer,silero_vad_analyzer]",
]

# recorder module tag -> pkgs
rms_recorder = []
vad_recorder = ["achatbot[speech_vad]"]

# --------------------------------- llm --------------------------
# llm engine
# llm module tag -> pkgs
# init use cpu Pre-built Wheel to install, 
# if want to use other lib(cuda), see: https://github.com/abetlen/llama-cpp-python#installation-configuration
llama_cpp = ["llama-cpp-python~=0.2.82"]
llm_personalai_proxy = ["geocoder~=1.38.1"]
sglang = ["sglang[all]==0.4.4.post1"]
vllm = ["vllm==0.7.3"]
transformers = ["transformers[torch]"]
# --extra-index-url https://pypi.nvidia.com python3.10 and python3.12 | python3.11 unsupported
trtllm = ["tensorrt-llm==0.17.0.post1"]

# llm backend
# --extra-index-url "https://flashinfer.ai/whl/cu124/torch2.5",
flashinfer-python = ["flashinfer-python==0.2.3"]


# vision llm
llm_transformers_manual_vision = [
    #"transformers@git+https://github.com/huggingface/transformers",
    "transformers",
    "qwen-vl-utils",
    "av",
    "achatbot[torch_vision_audio]",
]
llm_transformers_manual_vision_qwen = [
    "achatbot[llm_transformers_manual_vision]",
]
llm_transformers_manual_vision_llama = [
    "achatbot[llm_transformers_manual_vision]",
]
llm_transformers_manual_vision_molmo = [
    "achatbot[llm_transformers_manual_vision,einops]",
]
vision_transformers_got_ocr = [
    "achatbot[llm_transformers_manual_vision,tiktoken,verovio,accelerate]",
]
llm_transformers_manual_vision_img_janus = [
    "achatbot[llm_transformers_manual_vision,accelerate,einops]",
    "sentencepiece",
    "attrdict",
    "timm>=0.9.16",
]
llm_transformers_manual_vision_deepseekvl2 = [
    "achatbot[accelerate,einops]",
    "transformers==4.38.2",        # https://github.com/deepseek-ai/DeepSeek-VL2/issues/4#issuecomment-2550928148
    "xformers>=0.0.21",
    "sentencepiece",
    "attrdict",
    "timm>=0.9.16",
]
llm_transformers_manual_vision_kimi = [
    "numpy==1.26.2",
    "blobfile",
    "achatbot[llm_transformers_manual_vision,tiktoken,accelerate]",
]
llm_transformers_manual_vision_fastvlm = [
    "achatbot[torch_vision_audio,accelerate]",
    "numpy==1.26.4",
    "transformers==4.48.3",
    "tokenizers==0.21.0",
    "sentencepiece==0.1.99",
    "shortuuid",
    "peft>=0.10.0,<0.14.0",
    "bitsandbytes",
    "markdown2[all]",
    "scikit-learn==1.2.2",
    "einops==0.6.1",
    "einops-exts==0.0.4",
    "timm==1.0.15",
]
llm_transformers_manual_vision_smolvlm = [
    "achatbot[llm_transformers_manual_vision]",
    "num2words",
]
llm_transformers_manual_vision_gemma = [
    "achatbot[llm_transformers_manual_vision,accelerate]",
]
llm_transformers_manual_vision_mimo = [
    "achatbot[llm_transformers_manual_vision]",
]
llm_transformers_manual_vision_keye = [
    "keye-vl-utils[decord]==1.0.0",
    "achatbot[llm_transformers_manual_vision]",
]
llm_transformers_manual_vision_glm4v = [
    "achatbot[llm_transformers_manual_vision]",
]

# vision + audio -> text
llm_transformers_manual_vision_speech_phi = [
    "achatbot[torch_vision_audio,accelerate,soundfile]",
    "transformers==4.48.2",
    "backoff",
    "peft",
    "qwen-omni-utils",
]

# voice llm
llm_transformers_manual_voice = [
    #"transformers@git+https://github.com/huggingface/transformers",
    # https://github.com/huggingface/transformers/releases/tag/v4.45.2
    "transformers~=4.45.2",
    "torch~=2.2.2",
    "torchaudio~=2.2.2",
]
llm_transformers_manual_voice_glm = [
    "achatbot[llm_transformers_manual_voice,tts_cosy_voice,gdown,matplotlib,conf]",
]
llm_transformers_manual_voice_freeze_omni = [
    "achatbot[llm_transformers_manual_voice,librosa,soundfile,yaml]",
]
# speech llm
llm_transformers_manual_speech_llasa = [
    "achatbot[llm_transformers_manual_voice]",
]
llm_transformers_manual_speech_spark = [
    "achatbot[llm_transformers_manual_voice]",
]
llm_transformers_manual_speech_llama = [
    "achatbot[llm_transformers_manual_voice]",
]
# vision voice llm
llm_transformers_manual_vision_voice_minicpmo = [
    "achatbot[accelerate,librosa,soundfile]",
    "torch~=2.2.2",
    "torchaudio~=2.2.2",
    "torchvision~=0.17.2",
    "transformers==4.44.2",
    #"librosa==0.9.0",
    #"soundfile==0.12.1",
    "vector-quantize-pytorch~=1.18.5",
    "vocos~=0.1.0",
    "decord",
    "moviepy",
]
llm_transformers_manual_vision_voice_qwen = [
    # https://github.com/huggingface/transformers/releases/tag/v4.51.3-Qwen2.5-Omni-preview
    #"git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview",
    #"transformers==4.52.0",
    "achatbot[accelerate,librosa,soundfile]",
    "torch~=2.6.0",
    "torchaudio~=2.6.0",
    "torchvision~=0.21.0",
    "numpy==1.26.2",
    "qwen-omni-utils[decord]",
    # code2wav
    "torchdiffeq",
    "x_transformers",
]

llm_transformers_manual_voice_kimi = [
    "achatbot[accelerate,librosa,soundfile,conf]",
    "torch~=2.6.0",
    "torchaudio~=2.6.0",
    "torchdyn==1.0.6",
    "transformers",
    "pandas",
    "openai-whisper",
    "sox",
    "six==1.16.0",
    "hyperpyyaml",
    "conformer==0.3.2",
    "diffusers",
    "loguru",
    "tqdm",
    "huggingface_hub",
    "blobfile",
    "timm",
]
llm_transformers_manual_voice_vita = [
    "achatbot[torch_vision_audio,accelerate,librosa,soundfile,conf]",
    # https://github.com/huggingface/transformers/tags
    "transformers",     # use latest version, preview can see some new models
    "tiktoken",
    "funasr",
    "rich",
    "hyperpyyaml",
    "conformer",
    "lightning",
    "wget",
    "natsort",
    "safetensors",
    "diffusers",
    "gdown",
    "jiwer",
    "zhon",
    "WeTextProcessing",
    "inflect",
    "openai-whisper",
    "onnxruntime",
    "modelscope",
    "word2number",
    "pyworld",
    "matplotlib",
]


# core llms
core_llm = ["achatbot[llama_cpp,llm_personalai_proxy]"]

# ----------------- asr ------------------
# asr module tag -> pkgs
whisper_asr = ["openai-whisper"]
whisper_timestamped_asr = ["whisper-timestamped"]
whisper_faster_asr = ["faster-whisper"]
whisper_transformers_asr = ["transformers[torch]>=4.40.2"]
whisper_mlx_asr = [
    "mlx_whisper~=0.2.0; sys_platform == 'darwin' and platform_machine == 'arm64'",
]
whisper_groq_asr = ["groq~=0.9.0"]
sense_voice_asr = [
    "torch",
    "torchaudio",
    "funasr",
    "onnx",
    "onnxconverter-common",
]
speech_asr = [
    "achatbot[whisper_asr,whisper_timestamped_asr,whisper_faster_asr,whisper_transformers_asr,whisper_mlx_asr,whisper_groq_asr,sense_voice_asr]",
]

# -----------------codec------------------
# https://huggingface.co/kyutai/mimi/blob/main/config.json transformers_version
codec_transformers_mimi = ["transformers[torch]~=4.45.1"]
codec_moshi_mimi = ["moshi~=0.1.0"]
codec_xcodec2 = ["xcodec2==0.1.3"]
codec_transformers_dac = ["transformers[torch]~=4.45.1"]
codec_bitokenizer = [
    "einops==0.8.1",
    "einx==0.3.0",
    "numpy==2.2.3",
    "omegaconf==2.3.0",
    "packaging==24.2",
    "safetensors==0.5.2",
    "soundfile==0.12.1",
    "soxr==0.5.0.post1",
    "torch==2.5.1",
    "torchaudio==2.5.1",
    "torchvision==0.20.1",
    "transformers==4.46.2",
]
codec_snac = ["snac"]
codec_wavtokenizer = [
    "numpy==1.23.5",
    "encodec",
    "pyyaml",
    "huggingface_hub",
    "achatbot[einops,librosa,matplotlib,soundfile]",
]

# ---------------TTS----------------------

# tts module tag -> pkgs
tts_coqui = ["TTS~=0.22.0"]
tts_edge = ["edge-tts~=6.1.12"]
tts_g = ["gTTS~=2.5.1"]
tts_pyttsx3 = ["pyttsx3~=2.90"]
tts_cosy_voice = [
    "torch~=2.2.2",
    "torchaudio~=2.2.2",
    "transformers~=4.40.2",
    "hyperpyyaml~=1.2.2",
    "onnxruntime~=1.18.1",
    "openai-whisper==20231117",
    "WeTextProcessing~=1.0.2; sys_platform == 'linux'",
    "conformer~=0.3.2",
    "diffusers[torch]~=0.30.0",
    "lightning~=2.2.4",
    "wget~=3.2",
    "modelscope~=1.16.0",
    "achatbot[conf]",
]
tts_cosy_voice2 = ["achatbot[tts_cosy_voice]"]
tts_chat = [
    "torch~=2.2.2",
    "vocos~=0.1.0",
    "pybase16384~=0.3.7",
    "vector_quantize_pytorch~=1.16.1",
    "pynini~=2.1.5; sys_platform == 'linux'",
    "WeTextProcessing~=1.0.2; sys_platform == 'linux'",
    "nemo_text_processing~=1.0.2; sys_platform == 'linux'",
    "transformers~=4.40.2",
]
tts_f5 = [
    "wandb",                                                                            # for training
    "ema_pytorch",                                                                      # for training
    "datasets",                                                                         # for training
    "accelerate>=0.33.0",
    "bitsandbytes>0.37.0; platform_machine != 'arm64' and platform_system != 'Darwin'",
    "tomli",
    "cached_path",
    "click",
    "torch~=2.2.2",
    "torchaudio~=2.2.2",
    "matplotlib",
    "numpy<=1.26.4",
    "torchdiffeq",
    "jieba",
    "pypinyin",
    "achatbot[librosa,soundfile]",
    "transformers~=4.40.2",
    "x_transformers>=1.31.14",
    "vocos~=0.1.0",
]
tts_openvoicev2 = [
    "achatbot[librosa,soundfile,whisper_timestamped_asr,whisper_faster_asr]",
    "wavmark==0.0.3",
    "eng_to_ipa~=0.0.2",
    "inflect~=7.0.0",
    "unidecode~=1.3.7",
    "jieba",
    "pypinyin",
    "cn2an",
    "langid",
    #"melotts@git+https://github.com/myshell-ai/MeloTTS.git",
]
tts_kokoro = [
    #apt-get -qq -y install espeak-ng > /dev/null 2>&1
    "torch~=2.2.2",
    "transformers~=4.40.2",
    "phonemizer",
    "munch",
]
tts_onnx_kokoro = ["kokoro-onnx~=0.2.5"]
tts_fishspeech = [
    "torch~=2.3.1",                     # for torch.nn.attention with flash attention
    "torchaudio~=2.3.1",
    "transformers~=4.40.2",
    "natsort>=8.4.0",
    "loguru>=0.6.0",
    "rich>=13.5.3",
    "vector_quantize_pytorch==1.14.24",
    "loralib>=0.1.2",
    "tiktoken>=0.8.0",
    "pytorch-lightning~=2.4.0",
    "lightning~=2.4.0",
    "pyrootutils",
    "achatbot[conf,einops,librosa]",
]
tts_llasa = ["achatbot[codec_xcodec2]"]
tts_zonos = [
    "torch>=2.5.1",
    "inflect>=7.5.0",
    "kanjize>=1.5.0",
    "phonemizer>=3.3.0",
    "sudachidict-full>=20241021",
    "sudachipy>=0.6.10",
    "torchaudio>=2.5.1",
    "transformers>=4.48.1",       # use 4.45.1
    "huggingface-hub>=0.28.1",
    "achatbot[soundfile]",
]
tts_zonos_hybrid = [
    "achatbot[tts_zonos]",
    "flash-attn>=2.7.3",
    "mamba-ssm>=2.2.4",
    "causal-conv1d>=1.5.0.post8",
]
tts_step = [
    "torch==2.3.1",
    "torchaudio==2.3.1",
    "torchvision==0.18.1",
    "transformers==4.48.3",
    "accelerate==1.3.0",
    "openai-whisper==20231117",
    "sox==1.5.0",
    "modelscope",
    "six==1.16.0",
    "hyperpyyaml",
    "conformer==0.3.2",
    "diffusers",
    "onnxruntime-gpu==1.20.1",  # cuda 12.5
    "sentencepiece",
    "funasr>=1.1.3",
    "protobuf==5.29.3",
    "achatbot[conf,librosa]",
]
tts_spark = ["achatbot[codec_bitokenizer]"]
tts_generator_spark = ["achatbot[tts_spark]"]
tts_orpheus = [
    "achatbot[codec_snac]",
    "numpy==1.26.4",
    "torch==2.3.1",
    "torchaudio==2.3.1",
    "transformers==4.48.3",
]
tts_mega3 = [
    "torch==2.3.1",
    "torchaudio==2.3.1",
    "transformers==4.49.0",
    "WeTextProcessing==1.0.4.1",
    "pyloudnorm==0.1.1",
    "x-transformers==1.44.4",
    "torchdiffeq==0.2.5",
    "openai-whisper==20240930",
    "langdetect",
    "attrdict",
    "setproctitle==1.3.3",
    "achatbot[librosa]",
]


# multi tts modules engine
speech_tts = [
    "achatbot[tts_coqui,tts_edge,tts_g,tts_pyttsx3,tts_cosy_voice,tts_chat,tts_f5,tts_openvoicev2,tts_kokoro]",
]


# player module tag -> pkgs
stream_player = []

# vision detector
vision_yolo_detector = ["ultralytics~=8.3.12", "supervision~=0.24.0"]

# data process
pytube = ["pytube~=15.0.0"]
deep_translator = ["deep_translator~=1.11.4"]


# -----------------processor------------------
# when use asr tts processor 
#   e.g.: 
#   - achatbot[daily_rtvi_bot,deepgram_asr_processor,cartesia_tts_processor]
# when use vad asr tts engine: achatbot[daily_rtvi_bot,speech_vad_analyzer,asr_processor,tts_processor]

# ai framework
ai_langchain_framework_processor = ["langchain~=0.3.9"]
# multi ai framework modules processor
ai_frameworks_processor = ["achatbot[ai_langchain_framework_processor]"]

# avatar
lite_avatar = [
    "funasr",
    "av",
    "h5py",
    "jieba",
    "pypinyin",
    "transformers",
    "typeguard==2.13.3",
    "vector-quantize-pytorch",
    "vocos",
    "onnxruntime",
    "numpy==1.26.4",
]
lite_avatar_gpu = ["achatbot[lite_avatar]", "onnxruntime-gpu"]
musetalk_avatar = [
    "ffmpeg-python>=0.2.0",
    "imageio[ffmpeg]>=2.37.0",
    #"numpy==1.23.5",
    "tensorflow==2.12.0",
    "accelerate==0.32.0",
    #"torch==2.4.1",
    #"torchvision==0.19.1",
    #"torchaudio==2.4.1",
    "transformers==4.44.1",
    "av",
    "moviepy",
    "diffusers",
    "achatbot[conf,opencv,einops,librosa,soundfile]",
]
lam_audio2expression_avatar = [
    #"spleeter==2.4.2",
    "opencv_python_headless",
    "omegaconf",
    "addict==2.4.0",
    "yapf==0.40.1",
    "librosa",
    "termcolor",
    "numpy==1.24.3",
    "protobuf==5.29.4",
    "transformers==4.36.2",
]

# asr
deepgram_asr_processor = ["deepgram-sdk~=3.7.7"]
# multi asr modules processor
asr_processor = ["achatbot[deepgram_asr_processor,speech_asr]"]

# tts
cartesia_tts_processor = ["websockets~=12.0"]
# multi tts modules processor
tts_processor = ["achatbot[cartesia_tts_processor,openai,speech_tts]"]

# text llm processor
openai_llm_processor = ["achatbot[openai]"]
google_llm_processor = ["achatbot[google_ai,openai]"]
litellm_processor = ["achatbot[openai,litellm]"]
# multi api text llm modules processor
llm_processor = [
    "achatbot[openai_llm_processor,google_llm_processor,litellm_processor]",
]

# image processor
img_processor = ["openai~=1.54.1"]

# voice processor
glm_voice_processor = [
    "achatbot[llm_transformers_manual_voice_glm,bitsandbytes]",
]
freeze_omni_voice_processor = [
    "achatbot[llm_transformers_manual_voice_freeze_omni]",
]
moshi_voice_processor = ["moshi~=0.2.1"]
step_voice_processor = ["achatbot[tts_step]"]


#------------------------bot-------------------------

# default daily_rtvi_bot
# whisper_groq_asr asr engine
# like openai llm
# tts_edge tts engine
daily_rtvi_bot = [
    "achatbot[daily_transport,whisper_groq_asr,llm_processor,tts_edge]",
]

# default daily_langchain_rag_bot
langchain_openai_tidb_vector = [
    "achatbot[ai_langchain_framework_processor]",
    "langchain-text-splitters~=0.3.2",
    "langchain-openai~=0.2.11",
    "langchain-community~=0.3.9",                 # langchain.community.embeddings use Jina embeddings
    "tidb-vector~=0.0.10",                        # TIDB vss
    "pymysql~=1.1.1",                             # mysql py client
]
daily_langchain_rag_bot = [
    "achatbot[daily_transport,whisper_groq_asr,llm_processor,tts_edge,langchain_openai_tidb_vector]",
]

local_terminal_chat_bot = [
    "tqdm>=4.66.0",
    "achatbot[pyaudio_stream,speech_waker,speech_vad,speech_asr,core_llm,speech_tts,stream_player]",
]

daily_webrtc_terminal_chat_bot = [
    "achatbot[daily_room_audio_stream,speech_waker,speech_vad,speech_asr,core_llm,speech_tts,stream_player]",
]

remote_queue_chat_bot_fe = ["achatbot[speech_audio_stream,queue,stream_player]"]

remote_queue_chat_bot_be_worker = [
    "achatbot[queue,speech_waker,speech_vad,speech_asr,core_llm,speech_tts]",
]

remote_rpc_chat_bot_fe = ["achatbot[speech_audio_stream,rpc]"]
remote_rpc_chat_bot_be_worker = [
    "achatbot[rpc,speech_waker,speech_vad,speech_asr,core_llm,speech_tts]",
]

remote_grpc_tts_client = ["achatbot[grpc,stream_player]"]
remote_grpc_tts_server = ["achatbot[grpc,speech_tts]"]


# --------------------test---------------------------
test = ["sentence_transformers~=3.0.0", "pytest~=8.3.2", "pytest-mock~=3.14.0"]


# if use library, need add achatbot dir in pypi_build/app dir, change import codes
[tool.setuptools.packages.find]
# !NOTE: packages find .py file, other file don't to exclude
# All the following settings are optional:
where = ["pypi_build/app"]
#include = ["deps", "src", "tests"]
exclude = []


[tool.pytest.ini_options]
pythonpath = ["tests"]
#include = ["tests"]

[tool.setuptools_scm]
local_scheme = "no-local-version"


# https://docs.astral.sh/ruff/configuration/
[tool.ruff]
exclude = ["*_pb2.py", "*_pb2_*", "deps", "idl"]
indent-width = 4
line-length = 100

[tool.ruff.format]
exclude = ["*.pyi", "*.ipynb"]
[tool.ruff.lint]
ignore = ["F401", "F403", "F405", "F541", "E741"]

# uv
[tool.uv]
no-build-isolation-package = ["flash-attn"]
